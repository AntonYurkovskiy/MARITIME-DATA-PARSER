# –†–∞–±–æ—Ç–∞ —Å —Å–µ—Ä–≤–µ—Ä–æ–º
import requests
from pprint import pprint
import json

# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
import zipfile
import pandas as pd
import io
from bs4 import BeautifulSoup
from datetime import datetime


from urllib.parse import urljoin, urlparse
import os
import shutil
import base64
import re

from functools import lru_cache
from dotenv import load_dotenv

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

load_dotenv()

# @lru_cache(maxsize=1)  
def _get_session():
    """–ï–¥–∏–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    session = requests.Session()
    
    # ‚úÖ Retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: 3 –ø–æ–ø—ã—Ç–∫–∏ —Å backoff
    retry_strategy = Retry(
        total=10,
        backoff_factor=1,  # 1s, 2s, 4s –∑–∞–¥–µ—Ä–∂–∫–∏
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    login_data = {
        "email": os.getenv("CREWING_EMAIL"),
        "password": os.getenv("CREWING_PASSWORD"),
        "forced": True
    }
    
    headers = {'Content-Type': 'application/json'}
    
    # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: timeout + –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫!
    try:
        login_response = session.post(
            'https://staffdev.360crewing.com/api/v1/auth/login', 
            json=login_data, 
            headers=headers,
            timeout=(30, 60)  # 10s –∫–æ–Ω–Ω–µ–∫—Ç, 30s –æ—Ç–≤–µ—Ç
        )
        login_response.raise_for_status()
    except requests.exceptions.Timeout:
        print("‚è∞ TIMEOUT: staffdev.360crewing.com –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç!")
        print("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, VPN, firewall")
        raise
    except requests.exceptions.ConnectionError as e:
        print(f"üåê ConnectionError: {e}")
        raise
    except Exception as e:
        print(f"‚ùå Login failed: {e}")
        raise
    
    # ‚úÖ –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å–µ—Å—Å–∏–∏
    token = login_response.json().get("access_token")
    if not token:
        raise ValueError("–ù–µ—Ç access_token –≤ –æ—Ç–≤–µ—Ç–µ!")
        
    session.headers.update({
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
        'Accept': 'application/json',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    # print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
    return session


def add_value_in_dict(value: str, dict_name: str) -> dict:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å 360Crew API"""
    session = _get_session()  # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è
    url = f'https://staffdev.360crewing.com/api/v1/admin/dicts/{dict_name}'
    
    response = session.post(url, json={"value": value})
    response.raise_for_status()
    return response.json()

# –ü–û–õ–£–ß–ï–ù–ò–ï –°–õ–û–í–ê–†–Ø –ü–û –ö–õ–Æ–ß–£

@lru_cache(maxsize=128) 

def get_dict(key):
    session = _get_session()  # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è
    domain = 'https://staffdev.360crewing.com/api/v1/dict/'
    url = domain + key
    
    try:
        response = session.get(url, timeout=(10, 30))
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è {key}: {e}")
        raise


# –ü–û–õ–£–ß–ï–ù–ò–ï –í–°–ï–• –°–õ–û–í–ê–†–ï–ô
def get_dicts_list(is_static=False):
    
    session = _get_session() 
    
    url = f'https://staffdev.360crewing.com/api/v1/admin/dicts?is_static={is_static}'
    
    response = session.get(url)
    response.raise_for_status()
    data = response.json()
    return data

def search_geo(search_term: str, geo_type: str = "countries") -> list:
    """–ü–æ–∏—Å–∫ –≤ geo —Å–ª–æ–≤–∞—Ä—è—Ö"""
    session = _get_session()
    
    base_url = 'https://staffdev.360crewing.com/api/v1/dict'
    if geo_type in ['countries','cities','regions']:
        url = f'{base_url}/geo/{geo_type}/search/{search_term}'
    else:
        url = f'{base_url}/{geo_type}/search/{search_term}'
    
    # print(f"üîç GET {url}")
    response = session.get(url)  
    
    # print(f"Status: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        # print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(data)} –∑–∞–ø–∏—Å–µ–π")
        return data
    else:
        print(f"‚ùå {response.text}")
        return []

# –ø–æ–ª—É—á–µ–Ω–∏–µ ID
def get_id(dictionary,key):
    id = next((item['id'] for item in dictionary if item['value'].lower() == key.lower()), None)
    return id if id else add_value_in_dict(key,dictionary)['inserted']['id']
    

# –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨
def get_html_content(file_path):
    """
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç bs4 –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏
    
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    soup = BeautifulSoup(html_content, 'html.parser') 
    return soup


# –ü–ê–†–°–ï–† –§–û–¢–û
def get_photo_simple(soup):
    """–ü–∞—Ä—Å–∏—Ç —Ñ–æ—Ç–æ –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–ª—É—à–∫–∏

    Args:
        soup (bs4.BeautifulSoup): html —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é BeautifulSoup(html_content, 'html.parser')

    Returns:
        str: mime_type –≤–∏–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (image/jpeg, image/png, ...)
        base64: img_data –±–∏—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
    """
      
    src = soup.find('td', class_ = 'cvAvatar').find('img').get('src')
    header, data64 = src.split(',', 1)
    mime_type = header.split(';')[0].split(':')[1]  # image/jpeg
    ext = mime_type.split('/')[1]
    
    if not data64.startswith('iVBORw0KGgoAAAANSUhEUgAAARgAAAEZCAY'):
        img_data = base64.b64decode(data64)
        return mime_type, img_data
    else:
        return None   



# –û–°–ù–û–í–ù–û–ô –ü–ê–†–°–ï–† 
def main_parser(soup):
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã (–∫—Ä–æ–º–µ Notes) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª–æ–≤–∞—Ä—å 

    Args:
        soup (bs4.BeautifulSoup): html —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é BeautifulSoup(html_content, 'html.parser')

    Returns:
        dict: –û–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å –ø–æ –≤—Å–µ–º —Ç–∞–±–ª–∏—Ü–∞–º 
    """
    
    all_sections = {}
    
    tables = soup.find_all('table')
    
    for table in tables:
        title_row = table.find('tr', class_='cv-title')
        if title_row:
            
            table_title = title_row.get_text(strip=True)
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ç–∞–±–ª–∏—Ü—ã 'Main info','Additional info':
            if table_title in ['Main info','Additional info']:
               
                # –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–ª—é—á–∏
                keys = [
                        text_cleaning(td.get_text(strip=True))
                        for td
                        # —Ç–∞–±–ª–∏—Ü—É  'Main info' –Ω–∞—Ö–æ–¥–∏–º –ø–æ –∫–ª–∞—Å—Å—É
                        in table.find_all('td', class_='col-title')
                        # in soup.find('table', class_='cv-body').find_all('td', class_='col-title')
                        ]
                # –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è
                values = [
                
                        [text_cleaning(div.get_text(strip=True)) for div in td.find_all('div')]
                        if td.find_all('div') else text_cleaning(td.get_text(strip=True))
                    for td
                    in table.find_all('td', class_='cv-content')
                    # in soup.find('table', class_='cv-body').find_all('td', class_='cv-content')
                    ]
                # –ø–∞–∫—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
                section_data = dict(zip(keys,values))
                
                all_sections[table_title] = section_data
                    
            
            # –ø–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü—ã Biometrics
            elif table_title == 'Biometrics':
                section_data = {}
                rows = table.find_all('tr')
                for row in rows[1:]:
                    cells = row.find_all('td')
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        if len(text.split(':'))>1:
                            section_data[text.split(':')[0]] = text.split(':')[1]
                        else:
                            section_data[text.split(':')[0]] = None
            
                all_sections[table_title] = section_data
            
            # –ø–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –î–û–ö–£–ú–ï–ù–¢–´ –ò –°–¢–ê–ñ
            else:
                rows = table.find_all('tr')
                if len(rows) < 2:
                    all_sections[table_title] = None
                    continue
            
                # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –ò–ó –ü–ï–†–í–û–ô —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö (rows[1])
                headers = [th.get_text(strip=True) for th in rows[1].find_all(['td', 'th'])]
                if not headers:
                    all_sections[table_title] = None
                    continue
            
                # –î–∞–Ω–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—è —Å–æ –í–¢–û–†–û–ô —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö (rows[2:])
                section_data = []
                for row in rows[2:]:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) == len(headers):
                        row_dict = dict(zip(headers, [text_cleaning(cell.get_text(strip=True)) for cell in cells]))
                        section_data.append(row_dict)

                all_sections[table_title] = section_data     
 
    return all_sections

# –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê
def text_cleaning(raw_text):
    """–æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–ø–µ—Ü —Å–∏–º–≤–æ–ª–æ–≤

    Args:
        raw_text (str): –Ω–µ–æ—á–∏—â–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞

    Returns:
        str: –æ—á–∏—â–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
    """
    text = re.sub(r'[^\x20-\x7E–∞-—è–ê-–Ø—ë–Å\s]+|\s+', ' ', raw_text.strip())
    return text

# –ü–ê–†–°–ï–† –¢–ï–ö–°–¢–ê –ó–ê–ú–ï–¢–û–ö
def parse_notes(soup):
    """–ü–∞—Ä—Å–∏—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –ø–æ–ª–µ Notes

    Args:
        soup (bs4.BeautifulSoup): .html —Ñ–∞–π–ª –∏–ª–∏ –µ–≥–æ —á–∞—Å—Ç—å –ø–µ—Ä–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π –≤ bs.4

    Returns:
        str: —Ç–µ–∫—Å—Ç –∏–∑ –ø–æ–ª—è Notes
    """
    tables = soup.find_all('table')
    
    for table in tables:
        title_row = table.find('tr',class_ ='cv-title')
        if title_row and title_row.get_text(strip=True) == 'Additional info':
            
            rows = table.find_all('tr')
            
            for row_idx, row in enumerate(rows):
                if row.get('class') and 'cv-title' in row.get('class', []):
                    if row.get_text(strip=True) =='Notes':
                        if row_idx + 1 < len(rows):
                            value_row = rows[row_idx + 1]
                            return value_row.get_text(strip=True).replace('\n', ' ')
                        else:
                            return None


# –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•

# –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã
def only_letters_regex(text):
    result = re.sub(r'[^–∞-—è–ê-–Ø—ë–Åa-zA-Z]', '', text) if text else None
    return result if result else None

# –†–∞–∑–¥–µ–ª–∏—Ç—å –§–ò–û
def get_names(names_string):
    # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ –ø–µ—Ä–≤—ã–º –∏–¥–µ—Ç –∏–º—è
    name = only_letters_regex(names_string.split(' ')[0])
    # –≤—Å–µ —á—Ç–æ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –∑–¥–µ—Å—å
    middle_name = only_letters_regex(names_string.split(' ',1)[1].rsplit(' ',1)[0] if len(names_string.split(' ')) > 2 else None)
    # —Ñ–∞–º–∏–ª–∏—è –≤ –∫–æ–Ω—Ü–µ
    surname = only_letters_regex(names_string.split(' ')[-1]) 
    return name, middle_name, surname 
    

# –î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è –∏ –º–µ—Å—Ç–æ —Ä–æ–∂–¥–µ–Ω–∏—è
def get_birth_day_place(birth_day_place_string):
    if len(birth_day_place_string.split(' ',1))==2:
        day, place = birth_day_place_string.split(' ',1)
    return day, place

# –æ—á–µ—Ä–µ–¥–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def clean_letters_commas(text):    
    """
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –∑–∞–ø—è—Ç—ã–µ:
    - –£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –±—É–∫–≤ –∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç—ã–º–∏
    - –ü–æ—Å–ª–µ –∑–∞–ø—è—Ç—ã—Ö –æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –ø—Ä–æ–±–µ–ª
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    if not text:                   
        return None                 
    
    # –≠—Ç–∞–ø 1: –û—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –±—É–∫–≤—ã, –∑–∞–ø—è—Ç—ã–µ –∏ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'[^–∞-—è–ê-–Ø—ë–Åa-zA-Z,\s]', '', text)
         
    # –≠—Ç–∞–ø 2: –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç—ã–º–∏ (, )
    text = re.sub(r'\s+,', ',', text)
        
    # –≠—Ç–∞–ø 3: –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –±—É–∫–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç—ã–º–∏ (–±—É–∫–≤–∞ ,)
    text = re.sub(r'([–∞-—è–ê-–Ø—ë–Åa-zA-Z]),', r'\1,', text)
      
    # –≠—Ç–∞–ø 4: –ü–æ—Å–ª–µ –∑–∞–ø—è—Ç—ã—Ö ‚Üí 1 –ø—Ä–æ–±–µ–ª (–µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–∞)
    text = re.sub(r',([^ ])', r', \1', text)
                                   
    # –≠—Ç–∞–ø 5: –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    
    # –≠—Ç–∞–ø 6: –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
    result = text.strip()
    
    return result if result else None 

def extract_date_to_iso(text):     # –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏ –≤–µ—Ä–Ω—É—Ç—å ISO
    """
    –ò—â–µ—Ç –¥–∞—Ç—É –≤ —Å—Ç—Ä–æ–∫–µ, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ ISO (YYYY-MM-DD), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂:
    (iso_date –∏–ª–∏ None, –æ—Å—Ç–∞–ª—å–Ω–∞—è_—Å—Ç—Ä–æ–∫–∞_–±–µ–∑_–¥–∞—Ç—ã)
    """
    if not text:                   # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        return None         # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None 
    
    cleaned = text.strip()         # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–∞—Ç: DD.MM.YYYY, DD/MM/YYYY, DD-MM-YYYY
    date_pattern = r'\b(\d{1,2})[./-](\d{1,2})[./-](\d{4})\b'
                                   # \b - –≥—Ä–∞–Ω–∏—Ü–∞ —Å–ª–æ–≤–∞
                                   # (\d{1,2}) - –¥–µ–Ω—å (–≥—Ä—É–ø–ø–∞ 1)
                                   # [./-] - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                                   # (\d{1,2}) - –º–µ—Å—è—Ü (–≥—Ä—É–ø–ø–∞ 2)
                                   # (\d{4}) - –≥–æ–¥ (–≥—Ä—É–ø–ø–∞ 3)
    
    match = re.search(date_pattern, cleaned)  
                                   # –ò—â–µ–º –ø–µ—Ä–≤—É—é –¥–∞—Ç—É –≤ —Å—Ç—Ä–æ–∫–µ
    
    if not match:                  # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        return None, clean_letters_commas(cleaned)       # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None –∏ –≤—Å—é —Å—Ç—Ä–æ–∫—É
    
    day, month, year = match.groups()  
                                   # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ–Ω—å, –º–µ—Å—è—Ü, –≥–æ–¥ –∏–∑ –≥—Ä—É–ø–ø
    
    try:                           # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç—É
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
        date_obj = datetime(int(year), int(month), int(day))
                                   # int() –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–∞
                                   # datetime –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å (29.02 –≤ –Ω–µ–≤–∏—Å–æ–∫–æ—Å–Ω—ã–π)
        
        iso_date = date_obj.strftime('%Y-%m-%d')  
                                   # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ISO: YYYY-MM-DD
        
        # –£–¥–∞–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏
        start, end = match.span()  # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –¥–∞—Ç—ã –≤ —Å—Ç—Ä–æ–∫–µ
        remaining_text = cleaned[:start].strip() + ' ' + cleaned[end:].strip()
                                   # –û–±—Ä–µ–∑–∞–µ–º –¥–∞—Ç—É, —Å–∫–ª–µ–∏–≤–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫
        remaining_text = ' '.join(remaining_text.split())  
                                   # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        
        return iso_date, remaining_text



                                   # –í–æ–∑–≤—Ä–∞—â–∞–µ–º ISO –¥–∞—Ç—É –∏ –æ—Å—Ç–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç–∞
    
    except ValueError:             # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–∞ (32.13.2025)
        return None      # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None –∏ –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É

# –ø–æ–∏—Å–∫ –≤—Å–µ—Ö –µ–º–µ–π–ª–æ–≤
def find_emails(text):
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.findall(pattern, text)

def get_emails_list(email_string):
    """–ò–∑ —Å—Ç—Ä–æ–∫–∏ —Å –µ–º–µ–π–ª–∞–º–∏ –¥–µ–ª–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–æ —Ñ–æ—Ä–º–µ 
    {"email":"email", "comment":"comment", "uuid":"uuid"}

    Args:
        email_string (str): —Å—Ç—Ä–æ–∫–∞ –≤ –∫–æ—Ç–æ—Ä—É—é –¥–æ–ª–∂–µ–Ω –≤—Ö–æ–¥–∏—Ç—å email

    Returns:
        _type_: _description_
    """
    emails_list = []
    if find_emails(email_string):
        for email in find_emails(email_string):
            emails_list.append({"email":email, "comment":"comment", "uuid":None})
    else:
        return None
    return emails_list



def only_letters_digits_spaces(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç: —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ–±–µ–ª—ã"""
    if not text:                   # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Üí False
        return False
    # ^ –Ω–∞—á–∞–ª–æ, $ –∫–æ–Ω–µ—Ü, [] –ª—é–±–æ–π –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤
    pattern = r'^[–∞-—è–ê-–Ø—ë–Åa-zA-Z0-9\s]+$'
    return bool(re.match(pattern, text.strip()))


def get_personal_id_by_passport(pasports_list_of_dicts):
    priority_docs = ['International passport', 'National passport', "Seaman's book"]
    
    for doc_type in priority_docs:
        for doc in (pasports_list_of_dicts or []):
            if doc.get('Title of document') == doc_type:
                return doc['No.'] if only_letters_digits_spaces(doc['No.']) else None, doc_type
    
    return None, 'No documents'
   
def get_ranks(ranks):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –≤—Å–µ —Ä–∞–Ω–≥–∏ –ø–æ '/' –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫"""
    all_ranks = [part.strip() for rank in ranks for part in rank.split('/')]
    return all_ranks





